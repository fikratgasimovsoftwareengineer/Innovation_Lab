{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfa029e",
   "metadata": {},
   "source": [
    "# In this notebook, we have to do traning\n",
    "```with  PTH plus ON VisDrone as well as UAVDT dataset, because UAVDT dataset provides high depth resolution, in order to test our final model for test and validation```\n",
    "\n",
    "    Following Steps have to be observed:\n",
    "        1. Large YOLOV5 Model with weights of TPH PLUS\n",
    "        2. Training On Visdrone dataset\n",
    "            2.1 Img resolution L: 1536\n",
    "            2.2 Weeights TPH PLUS\n",
    "            2.3 Epoch 300\n",
    "            2.4 batch size 64-128\n",
    "        3. Visualization of Training Results\n",
    "        4. Testing and Validation on corresponding dataset\n",
    " # Second Training has to be executed on Our Custom dataset\n",
    "     1. Train our saved and trained model on our custom dataset\n",
    "          1. Img Resolution : 1280-1536\n",
    "          2. Weights : Saved Folder\n",
    "          3. Epoch - 250-300\n",
    "          4. Batch Size : 64 -128\n",
    "          5. Visualization of training results\n",
    "          4. Testing on Val Images\n",
    "          \n",
    " # If results are satisfactory, we need to make an inference for deployment\n",
    " \n",
    "      1. Convert Model to Onnx\n",
    "          1. Batch Size\n",
    "          2. WorkSpace\n",
    "          3. Onnx Model\n",
    "          4. Onnx Saved Path\n",
    "          5. Path to Yaml of classes\n",
    "          6. Validation of Onnx on test images\n",
    "          7. Salvare risultati di Onnx\n",
    "      2. Convert Model Onnx to Tensorrt\n",
    "          1. Dynamic Batch size\n",
    "          2. Workspcae for tensorrt\n",
    "          3. Tensor engine path \n",
    "          4. Precision \n",
    "          5. Validation on test data\n",
    "          6. Validation of Time of inference on images and video\n",
    "          7. Creation of Batch for cloud computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "214a505e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue May 23 08:39:02 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.41.03              Driver Version: 530.41.03    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3060 L...    Off| 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   31C    P0               N/A /  N/A|     10MiB /  6144MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74aa5c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = '/tensorfl_vision/Tensorflow_Yolov5/yolo/train/images'\n",
    "testing_data = '/tensorfl_vision/Tensorflow_Yolov5/yolo/val/images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41275f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = \"\"\"names:\n",
    "  - target\n",
    "  - target\n",
    "  - target\n",
    "nc: 3\n",
    "train: /tensorfl_vision/Tensorflow_Yolov5/yolo/train/images\n",
    "val: /tensorfl_vision/Tensorflow_Yolov5/yolo/val/images\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "with open('/tensorfl_vision/Tensorflow_Yolov5/yolo/vgg_tph_plus.yaml', 'w') as f:\n",
    "    f.write(yaml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e94b0ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'names': ['target', 'target', 'target'], 'nc': 3, 'train': '/tensorfl_vision/Tensorflow_Yolov5/yolo/train/images', 'val': '/tensorfl_vision/Tensorflow_Yolov5/yolo/val/images'}\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "data_yaml_2 = '/tensorfl_vision/Tensorflow_Yolov5/yolo/vgg_tph_plus.yaml'\n",
    "with open(data_yaml_2, 'r') as f:\n",
    "    data = yaml.safe_load(f)\n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "212c6a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 11:44:13.520977: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-24 11:44:13.555195: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) ^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 48, in <module>\n",
      "    from utils.loggers import Loggers\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/utils/loggers/__init__.py\", line 27, in <module>\n",
      "    wandb_login_success = wandb.login(timeout=30)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 77, in login\n",
      "    configured = _login(**kwargs)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 298, in _login\n",
      "    wlogin.prompt_api_key()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 221, in prompt_api_key\n",
      "    key, status = self._prompt_api_key()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_login.py\", line 201, in _prompt_api_key\n",
      "    key = apikey.prompt_api_key(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/apikey.py\", line 107, in prompt_api_key\n",
      "    result = prompt_choices(\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/util.py\", line 1207, in prompt_choices\n",
      "    choice = _prompt_choice(input_timeout=input_timeout, jupyter=jupyter)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/util.py\", line 1192, in _prompt_choice\n",
      "    choice = input_fn(text, jupyter=jupyter)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/timed_input.py\", line 120, in timed_input\n",
      "    return _timed_input(prompt=prompt, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/timed_input.py\", line 26, in _posix_timed_input\n",
      "    events = sel.select(timeout=timeout)\n",
      "  File \"/usr/lib/python3.8/selectors.py\", line 468, in select\n",
      "    fd_event_list = self._selector.poll(timeout, max_ev)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "sign_data=data_yaml_2\n",
    "!python /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py --img 1536 --batch 4 --epochs {epochs} --data {sign_data} --weights /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/yolov5l6.pt --name v5l-tph-plu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91b5215b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 11:47:19.999936: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-24 11:47:20.032359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/models/yolov5l-tph-plus.yaml, data=/tensorfl_vision/Tensorflow_Yolov5/yolo/vgg_tph_plus.yaml, hyp=tph-yolov5/data/hyps/hyp.scratch.yaml, epochs=50, batch_size=4, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=True, sync_bn=False, workers=8, project=tph-yolov5/runs/train, name=v5l-tph-plu, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "fatal: ambiguous argument 'main..origin/master': unknown revision or path not in the working tree.\n",
      "Use '--' to separate paths from revisions, like this:\n",
      "'git <command> [<revision>...] -- [<file>...]'\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mCommand 'git rev-list main..origin/master --count' returned non-zero exit status 128.\n",
      "YOLOv5 🚀 052dfeb torch 2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir tph-yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   5255696  models.common.C3STR                     [1024, 1024, 1, False]        \n",
      " 24   [2, 17, 20, 23]  1    138952  models.yolo.CLLADetect                  [3, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512, 1024]]\n",
      "Model Summary: 466 layers, 41528920 parameters, 41528920 gradients, 160.6 GFLOPs\n",
      "\n",
      "Transferred 570/600 items from yolov5l.pt\n",
      "Scaled weight_decay = 0.0005\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam with parameter groups 95 weight, 109 weight (no decay), 108 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.3, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.3, clip_limit=(1, 4.0), tile_grid_size=(8, 8)), RandomBrightnessContrast(always_apply=False, p=0.3, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '/tensorfl_vision/Tensorflow_Yolov5/yolo/train/labels.cache' ima\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/tensorfl_vision/Tensorflow_Yolov5/yolo/val/labels.cache' images \u001b[0m\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 3.94, Best Possible Recall (BPR) = 0.9672. Attempting to improve anchors, please wait...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 2 of 2775 labels are < 3 pixels in size.\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 2775 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 8.96 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.611/0.864-mean/best, past_thr=0.613-mean: 5,6,  9,6,  6,9,  9,9,  12,8,  8,13,  12,11,  17,12,  13,17\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.8673: 100%|█| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.25: 1.0000 best possible recall, 8.97 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=640, metric_all=0.626/0.867-mean/best, past_thr=0.627-mean: 5,6,  6,9,  9,7,  8,9,  12,8,  8,12,  11,10,  12,15,  16,12\n",
      "Reversing anchor order\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mtph-yolov5/runs/train/v5l-tph-plu5\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
      "  0%|                                                   | 0/205 [00:01<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 630, in <module>\n",
      "    main(opt)\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 527, in main\n",
      "    train(opt.hyp, opt, device, callbacks)\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 324, in train\n",
      "    loss, loss_items = compute_loss(pred, targets.to(device))  # loss scaled by batch_size\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/utils/loss.py\", line 142, in __call__\n",
      "    tcls, tbox, indices, anchors = self.build_targets(p, targets)  # targets\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/utils/loss.py\", line 240, in build_targets\n",
      "    indices.append((b, a, gj.clamp_(0, gain[3] - 1), gi.clamp_(0, gain[2] - 1)))  # image, anchor, grid indices\n",
      "RuntimeError: result type Float can't be cast to the desired output type long int\n"
     ]
    }
   ],
   "source": [
    "epochs=50\n",
    "sign_data=data_yaml_2\n",
    "!python /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py --img 640 --adam --batch 4 --epochs {epochs} --data {sign_data} --weights yolov5l.pt --name v5l-tph-plu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d068aa9",
   "metadata": {},
   "source": [
    "# Here operations hyperparamets and modes :\n",
    "\n",
    "\n",
    "    1. Model: yolov5l + tph plus weights\n",
    "    2. Epoch : 300\n",
    "    3. img size : 1536\n",
    "    4. activation function : adam\n",
    "    5. batch: 64\n",
    "    6. Hyperparameters: hyp.VisDrone.yaml \n",
    "    7.tph ++ weights: yolov5l-tph-plus.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a13060c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-05-24 11:46:00.288523: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-24 11:46:00.341013: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5l.pt, cfg=/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/models/yolov5l-tph-plus.yaml, data=/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/data/VisDrone.yaml, hyp=/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/data/hyps/hyp.VisDrone.yaml, epochs=300, batch_size=4, imgsz=1536, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=True, sync_bn=False, workers=8, project=tph-yolov5/runs/train, name=v5l-tph-plus, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "fatal: ambiguous argument 'main..origin/master': unknown revision or path not in the working tree.\n",
      "Use '--' to separate paths from revisions, like this:\n",
      "'git <command> [<revision>...] -- [<file>...]'\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0mCommand 'git rev-list main..origin/master --count' returned non-zero exit status 128.\n",
      "YOLOv5 🚀 052dfeb torch 2.0.1+cu117 CUDA:0 (NVIDIA GeForce RTX 3060 Laptop GPU, 5938MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0032, lrf=0.12, momentum=0.843, weight_decay=0.00036, warmup_epochs=2.0, warmup_momentum=0.5, warmup_bias_lr=0.05, box=0.07, cls=0.18, cls_pw=0.631, obj=0.15, obj_pw=0.911, iou_t=0.2, anchor_t=3, fl_gamma=0.0, hsv_h=0.4, hsv_s=0.3, hsv_v=0.5, degrees=0.2, translate=0.0, scale=0.4, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.2, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv5 🚀 runs (RECOMMENDED)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir tph-yolov5/runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=10\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]              \n",
      "  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  2                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
      "  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  4                -1  6   1118208  models.common.C3                        [256, 256, 6]                 \n",
      "  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  6                -1  9   6433792  models.common.C3                        [512, 512, 9]                 \n",
      "  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
      "  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]               \n",
      "  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]               \n",
      " 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]         \n",
      " 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]          \n",
      " 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]          \n",
      " 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   5255696  models.common.C3STR                     [1024, 1024, 1, False]        \n",
      " 24   [2, 17, 20, 23]  1    173959  models.yolo.CLLADetect                  [10, [[0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5], [0, 1, 2, 3, 4, 5]], [128, 256, 512, 1024]]\n",
      "Model Summary: 466 layers, 41563927 parameters, 41563927 gradients, 160.7 GFLOPs\n",
      "\n",
      "Transferred 570/600 items from yolov5l.pt\n",
      "Scaled weight_decay = 0.00036\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam with parameter groups 95 weight, 109 weight (no decay), 108 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.3, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.3, clip_limit=(1, 4.0), tile_grid_size=(8, 8)), RandomBrightnessContrast(always_apply=False, p=0.3, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/VisDrone/VisDrone2019-DET-train/labels.cache' image\u001b[0m\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/0000137_02220_d_0000163.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/0000140_00118_d_0000002.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/9999945_00000_d_0000114.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING: ../datasets/VisDrone/VisDrone2019-DET-train/images/9999987_00000_d_0000049.jpg: 1 duplicate labels removed\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/VisDrone/VisDrone2019-DET-val/labels.cache' images an\u001b[0m\n",
      "Plotting labels... \n",
      "\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mAnalyzing anchors... anchors/target = 0.40, Best Possible Recall (BPR) = 0.1131. Attempting to improve anchors, please wait...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mWARNING: Extremely small objects found. 625 of 343201 labels are < 3 pixels in size.\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mRunning kmeans for 9 anchors on 343201 points...\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.33: 0.9901 best possible recall, 3.98 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=1536, metric_all=0.336/0.723-mean/best, past_thr=0.537-mean: 11,13,  17,27,  38,23,  29,47,  72,39,  54,75,  133,73,  88,130,  215,182\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mEvolving anchors with Genetic Algorithm: fitness = 0.7507: 100%|█| 1\u001b[0m\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mthr=0.33: 0.9971 best possible recall, 4.55 anchors past thr\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mn=9, img_size=1536, metric_all=0.368/0.752-mean/best, past_thr=0.540-mean: 8,10,  10,21,  20,16,  19,33,  41,22,  34,47,  71,39,  65,82,  146,101\n",
      "\u001b[34m\u001b[1mautoanchor: \u001b[0mNew anchors saved to model. Update model *.yaml to use these anchors in the future.\n",
      "\n",
      "Image sizes 1536 train, 1536 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mtph-yolov5/runs/train/v5l-tph-plus5\u001b[0m\n",
      "Starting training for 300 epochs...\n",
      "\n",
      "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/1618 [00:00<?, ?it/s]\r",
      "  0%|                                                  | 0/1618 [00:02<?, ?it/s]\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 630, in <module>\r\n",
      "    main(opt)\r\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 527, in main\r\n",
      "    train(opt.hyp, opt, device, callbacks)\r\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py\", line 323, in train\r\n",
      "    pred = model(imgs)  # forward\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n",
      "    return forward_call(*args, **kwargs)\r\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/models/yolo.py\", line 249, in forward\r\n",
      "    return self._forward_once(x, profile, visualize)  # single-scale inference, train\r\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/models/yolo.py\", line 272, in _forward_once\r\n",
      "    x = m(x)  # run\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n",
      "    return forward_call(*args, **kwargs)\r\n",
      "  File \"/tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/models/common.py\", line 46, in forward\r\n",
      "    return self.act(self.bn(self.conv(x)))\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\r\n",
      "    return forward_call(*args, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/modules/batchnorm.py\", line 171, in forward\r\n",
      "    return F.batch_norm(\r\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/torch/nn/functional.py\", line 2450, in batch_norm\r\n",
      "    return torch.batch_norm(\r\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 72.00 MiB (GPU 0; 5.80 GiB total capacity; 3.92 GiB already allocated; 27.75 MiB free; 4.23 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n"
     ]
    }
   ],
   "source": [
    "!python /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/train.py --img 1536 --adam --batch 4 --epochs 300 --data /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/data/VisDrone.yaml --weights yolov5l.pt --hy /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/data/hyps/hyp.VisDrone.yaml --cfg /tensorfl_vision/Tensorflow_Yolov5/tph-yolov5/models/yolov5l-tph-plus.yaml --name v5l-tph-plus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c59a388",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
